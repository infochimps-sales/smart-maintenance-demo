## The Smart Maintenance Demo

by Joe Hahn,
joe.hahn@infochimps.com,
20 May 2015

This is the Github repository for the master branch of the Smart Maintenance Demo for Hadoop,
this demo uses python's scikit-learn machine-learning algorithm to perform predictive
maintenance on 200 simulated motors. This non-parallelized code executes in 3.8 minutes on
a hadoop-foyer node, a followup effort will get this running in parallel on the hadoop
datanodes using Spark.

###To install:

First clone this github repo to your home directory on the hadoop foyer node:

    cd; git clone git@github.com:infochimps-sales/spark-airline-demo.git 
    
   
Then execute the installer, this will download and install some python libraries to all 
hadoop nodes, and is done in takes 5 minutes:

    cd spark-airline-demo
    ./install.sh





1 ssh into the platform's hadoop foyer node:

     ssh -A joehahn@52.8.44.194
     ssh -A cdh-foyer
     

2 install Anaconda python, which provides a painless way to install nearly all of the python
  libraries to be used here

    wget http://09c8d0b2229f813c1b93-c95ac804525aac4b6dba79b00b39d1d3.r79.cf1.rackcdn.com/Anaconda-2.1.0-Linux-x86_64.sh 
    sudo bash Anaconda-2.1.0-Linux-x86_64.sh (and install into /opt/anaconda)


3 clone the smart maintenance demo

    git clone git@github.com:infochimps-sales/smart-maintenance-demo.git
    cd smart-maintenance-demo/source


4 execute the demo

    /opt/anaconda/bin/python smart_maint.py
    /home/$USER/anaconda/bin/python smart_maint.py


5 install screen & restart webserver

    sudo yum install screen
    screen -S webserver -X quit
    screen -S webserver -d -m sh -c "cd ~/smart-maintenance-demo/source/data;
        /opt/anaconda/bin/python -m SimpleHTTPServer 12321"
    /home/$USER/anaconda/bin/python -m SimpleHTTPServer 12321 > /dev/null 2>&1 &

6 browse the *.png images stored in http://cdh-foyer.platform.infochimps:12321
    

###Still To do:

---have the motor data land in Impala (rather than residing in memory), and then use
data generated by an Impala query to train the SVM predictor
  
---execute this simulation in parallel on a hadoop cluster, using hadoop-streaming
or Spark or H2o.


###Notes (in-progress) on parallelizing this using spark


execute demo using anaconda-python on spark:

    PYSPARK_PYTHON=/opt/anaconda/bin/python spark-submit smart_maint.py 


starting an ipython session on spark:

    PYSPARK_DRIVER_PYTHON=/opt/anaconda/bin/ipython pyspark
    
    
spark UI:

    http://cdh-foyer.platform.infochimps:4040/jobs/


### spark/yarn:
 
on Yarn, monitor the tracking URL, will be similar to 
http://cdh-rm.platform.infochimps:8088/proxy/application_1433888715374_0005/%20user:%20joehahn

MASTER=yarn-client pyspark --num-executors 3
browse http://cdh-foyer.platform.infochimps:4040/jobs/
>>> textFile = sc.textFile("hdfs://cdh-nn:8020/tmp/shakespeare/shakespeare.txt")
wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b).collect()
wordCounts.saveAsTextFile("wc")
wc_dict = dict(wordCounts)
wc_dict['shalt']
#sc.stop()


    
